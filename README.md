# Model Eval results

### openllm Benchmark results

| Model | Details | Average | arc | gsm8k | hellaswag | mmlu | truthfulqa | winogrande |
|---|---|---|---|---|---|---|---|---|
| [Arithmo-Wizard-2-7B](https://huggingface.co/saucam/Arithmo-Wizard-2-7B) | [complete result](saucam/Arithmo-Wizard-2-7B/README.md) | 65.22 | 62.88 | 61.26 | 83.15 | 60.61 | 45.9 | 77.51 |
| [Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) | [complete result](meta-llama/Meta-Llama-3-8B-Instruct/README.md) | 68.27 | 62.2 | 75.59 | 78.84 | 65.82 | 51.71 | 75.45 |
| [Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B) | [complete result](meta-llama/Meta-Llama-3-8B/README.md) | 62.84 | 57.51 | 50.87 | 82.09 | 65.04 | 43.93 | 77.58 |
| [Nereus-7B](https://huggingface.co/saucam/Nereus-7B) | [complete result](saucam/Nereus-7B/README.md) | 63.82 | 62.54 | 46.25 | 83.23 | 59.6 | 54.32 | 76.95 |
| [Orpomis-Prime-7B-dare](https://huggingface.co/saucam/Orpomis-Prime-7B-dare) | [complete result](saucam/Orpomis-Prime-7B-dare/README.md) | 67.37 | 64.68 | 59.74 | 85.12 | 62.21 | 53.72 | 78.77 |
| [Orpomis-Prime-7B-it](https://huggingface.co/saucam/Orpomis-Prime-7B-it) | [complete result](saucam/Orpomis-Prime-7B-it/README.md) | 55.67 | 61.26 | 24.34 | 79.61 | 51.55 | 43.68 | 73.56 |
| [Orpomis-Prime-7B](https://huggingface.co/saucam/Orpomis-Prime-7B) | [complete result](saucam/Orpomis-Prime-7B/README.md) | 55.27 | 60.67 | 24.41 | 79.12 | 52.43 | 41.02 | 73.95 |
| [mera-mix-4x7B](https://huggingface.co/meraGPT/mera-mix-4x7B) | [complete result](meraGPT/mera-mix-4x7B/README.md) | 76.59 | 71.76 | 72.93 | 88.92 | 63.8 | 77.6 | 84.53 |
| [mistral-orpo-beta-NeuralBeagle14-7B-dare-ties](https://huggingface.co/saucam/mistral-orpo-beta-NeuralBeagle14-7B-dare-ties) | [complete result](saucam/mistral-orpo-beta-NeuralBeagle14-7B-dare-ties/README.md) | 70.06 | 67.32 | 61.79 | 85.89 |  | 54.17 | 81.14 |


### nous Benchmark results

| Model | Details | Average | agieval | bigbench | gpt4all | truthfulqa |
|---|---|---|---|---|---|---|
| [Arithmo-Wizard-2-7B](https://huggingface.co/saucam/Arithmo-Wizard-2-7B) | [complete result](saucam/Arithmo-Wizard-2-7B/README.md) | 46.28 | 31.58 | 37.44 | 70.2 | 45.91 |
| [Nereus-7B](https://huggingface.co/saucam/Nereus-7B) | [complete result](saucam/Nereus-7B/README.md) | 52.12 | 42.8 | 39.17 | 72.21 | 54.32 |
| [Orpomis-Prime-7B-dare](https://huggingface.co/saucam/Orpomis-Prime-7B-dare) | [complete result](saucam/Orpomis-Prime-7B-dare/README.md) | 52.42 | 42.71 | 39.82 | 73.42 | 53.72 |
| [Orpomis-Prime-7B-it](https://huggingface.co/saucam/Orpomis-Prime-7B-it) | [complete result](saucam/Orpomis-Prime-7B-it/README.md) | 47.98 | 37.23 | 38.72 | 72.28 | 43.68 |
| [Orpomis-Prime-7B](https://huggingface.co/saucam/Orpomis-Prime-7B) | [complete result](saucam/Orpomis-Prime-7B/README.md) | 46.78 | 36.4 | 37.5 | 72.18 | 41.02 |
| [bert-tiny-finetuned-sms-spam-detection](https://huggingface.co/mrm8488/bert-tiny-finetuned-sms-spam-detection) | [complete result](mrm8488/bert-tiny-finetuned-sms-spam-detection/README.md) | 33.97 | 22.95 | 28.75 | 36.07 | 48.09 |
